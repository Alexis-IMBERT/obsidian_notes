---
tags:
  - MLBrief
  - ENS
  - Softmax
  - self-Training
  - sample-selection-bias
  - workshop
  - article
---
## RÃ©sumÃ© :
Quand nous possÃ©dons un grand nombre de donnÃ©es, il est compliquÃ© de pouvoir labelliser l'ensemble des donnÃ©es. On peut alors appliquer des algorithmes d'auto-entrainement qui vont donner des pseudo-labels aux donnÃ©es non labellisÃ©es quand le modÃ¨le a une confiance suffisante.

Le problÃ¨me, c'est que le score de confiance actuellement utilisÃ© est la softmax qui a tendance Ã  Ãªtre trop confiante (mÃªme avec des faux positifs). L'article propose donc un nouveau score de confiance : la T-similiratÃ©. 

Le problÃ¨me vient principalement du fait de la pertinence des donnÃ©es labellisÃ© de base : des biais que les donnÃ©es prÃ©sentent, en effet, les donnÃ©es labellisÃ©es ne reprÃ©sentent pas forcÃ©ment l'ensemble des donnÃ©es labellisÃ©, il peut exister une diffÃ©rence de loi de distribution entre les donnÃ©es labellisÃ©es et les donnÃ©es non labellisÃ©es. Pour prouver l'efficacitÃ© de ce nouveau score de confiance, plusieurs expÃ©riences vont Ãªtre rÃ©alisÃ©es sur plusieurs jeux de donnÃ©es (rÃ©pÃ©tÃ©s avec 9 seeds diffÃ©rente) avec des donnÃ©es IndÃ©pendante et Identiquement DistribuÃ© (IID) et des donnÃ©es biaisÃ©es (SSB).

L'idÃ©e est de donner le score de similaritÃ© d'un exemple non Ã©tiquetÃ© Ã  partir de plusieurs rÃ©sultats de classifieurs divers. Si les classifieurs sont d'accord dans l'ensemble, alors le score de similaritÃ© sera Ã©levÃ©. Si les classifieurs ne sont pas d'accord, alors le score de similaritÃ© sera faible.

Les rÃ©sultats montrent bien que quand les donnÃ©es sont IID parmi toutes les donnÃ©es, la T-similiratÃ© n'a pas beaucoup d'utilitÃ© par rapport Ã  la softmax. Les rÃ©sultats entre la softmax et la T-sim sont proches, la softmax peut Ãªtre lÃ©gÃ¨rement meilleure. Par contre, quand les donnÃ©es sont biaisÃ©es, alors la softmax admet des rÃ©sultats d'apprentissage bien infÃ©rieurs par rapport Ã  la T-similaritÃ©.

![[2310.14814v4.pdf]]

https://arxiv.org/html/2310.14814v4

Self trainning : semi supervised learning (SSL)
-> assigner des pseudo label aux valeur non labÃ©lisÃ© dans lesquelle le modÃ¨les donne une confiance suffisante
-> le sueil peut etre fixe OU changÃ© Ã  chaque itÃ©ration -> **curriculum learning**
=> problÃ¨me quand le classifier est biaisÃ© ou quand la loi de distribution change entre les donnÃ©es labÃ©llisÃ© et non labÃ©lisÃ©


Softmax often use but known to be overconfident (even for wrong prediction)

So creation of a T-similarity
	- comme un ensemble de plusieurs prÃ©diction et si la majoritÃ© est d'accord alors c'est bon
[Lien du code](https://github.com/ambroiseodt/tsim)


(1) How to rank unlabeled data to reflect their difficulty for classification? 
 * less documented
(2) How to select the threshold for pseudo-labeling at each iteration?
 - well documented : (Feofanov etÂ al.,, [2019](https://arxiv.org/html/2310.14814v4#bib.bib19); Cascante-Bonilla etÂ al.,, [2021](https://arxiv.org/html/2310.14814v4#bib.bib9); Zhang etÂ al.,, [2021](https://arxiv.org/html/2310.14814v4#bib.bib63))
 
sample selection bias (SSB)
disparitÃ© de distribution entre les donnÃ©es Ã©tiquetÃ©e et les donnÃ©es non Ã©tiquetÃ© 
 FormalisÃ© par Heckman (1974), 
 Dans le contexte de la classification, la SSB a Ã©tÃ© correctement dÃ©finie par Zadrozny (2004)
 
We denote this challenging scenario by SSL + SSB.

DiversitÃ© de l'ensemble 
	dans les annÃ©e prÃ©cÃ©dente la diversitÃ© a Ã©tÃ© crÃ©er par le bagging
	>To this end, Liu and Yao, (1999), introduced a mixture of expert that are diversified through the negative correlation loss that forces a trade-off between specialization and cooperation of the experts
	
#### Datasets.

We consider 13 publicly available SSL datasets with various data modalities: 
* biological data for 
	* Cod-RNA (Chang and Lin,, [2011](https://arxiv.org/html/2310.14814v4#bib.bib10)), 
	* DNA (Chang and Lin,, [2011](https://arxiv.org/html/2310.14814v4#bib.bib10)), 
	* Protein (Dua and Graff,, [2017](https://arxiv.org/html/2310.14814v4#bib.bib17)), 
	* Splice (Dua and Graff,, [2017](https://arxiv.org/html/2310.14814v4#bib.bib17)); 
* images for 
	* COIL-20 (Nene etÂ al.,, [1996](https://arxiv.org/html/2310.14814v4#bib.bib49)), 
	* Digits (Pedregosa etÂ al.,, [2011](https://arxiv.org/html/2310.14814v4#bib.bib52)), 
	* Mnist (Lecun etÂ al.,, [1998](https://arxiv.org/html/2310.14814v4#bib.bib41)); 
* tabular data for 
	* DryBean (Dua and Graff,, [2017](https://arxiv.org/html/2310.14814v4#bib.bib17)), 
	* Mushrooms (Dua and Graff,, [2017](https://arxiv.org/html/2310.14814v4#bib.bib17)), 
	* Phishing (Chang and Lin,, [2011](https://arxiv.org/html/2310.14814v4#bib.bib10)), 
	* Rice (Dua and Graff,, [2017](https://arxiv.org/html/2310.14814v4#bib.bib17)), 
	* Svmguide1 (Chang and Lin,, [2011](https://arxiv.org/html/2310.14814v4#bib.bib10)); t
* ime series for 
	* HAR (Dua and Graff,, [2017](https://arxiv.org/html/2310.14814v4#bib.bib17)). 
* More details can be found in AppendixÂ [B.1](https://arxiv.org/html/2310.14814v4#A2.SS1 "B.1 Datasets â€£ Appendix B Experimental Setup"). All experimental results reported below are obtained over 9 different seeds.

#### Labeling procedure.

To generate SSL data, we consider the two following labeling strategies and compare the studied baselines in both cases (see more details in AppendixÂ [B.2](https://arxiv.org/html/2310.14814v4#A2.SS2 "B.2 More details on the labeling procedure â€£ Appendix B Experimental Setup")):

1. IID: this is the case usually considered in classification tasks and that verifies the i.i.d. assumption. The selection variable s is completely random, i.e., independent of ğ± and y. In this case, we have Pâ¢(s=1|ğ±,y)=Pâ¢(s=1) and thus Plabâ¢(ğ±,y)=Pâ¢(ğ±,y).
IID Par opposition Ã  SSB

3. SSB: in this case, s is dependent of ğ± and y. For each class c and each data ğ± with label y=c, we impose:

| Pâ¢(s=1\|ğ±,y=c)=1Î²â¢expâ¡(rÃ—\|proj1â¢(ğ±)\|), |
| ------------------------------------------ |
where r>0 is a hyperparameter, proj1â¢(ğ±) is the projection value of ğ± on the first principal component of the training data in class c and Î²=âˆ‘ğ±expâ¡(rÃ—|proj1â¢(ğ±)|) is a normalizing constant to ensure that P(s=1|ğ±âˆˆğ—â„“,y=c)=1. The impact of the strength of the bias is studied in AppendixÂ [C.5](https://arxiv.org/html/2310.14814v4#A3.SS5 "C.5 Robustness to the strength of the bias â€£ Appendix C Additional Experiments").

#### Architecture and training parameters.

In all of our experiments, we train a 3-layer MLP, with the Adam optimizer (Kingma and Ba,, [2015](https://arxiv.org/html/2310.14814v4#bib.bib38)) and a learning rate of 0.001. Unless specified otherwise, we consider M=5 linear heads and a diversity strength parameter Î³=1. In our experiments, we take â„“sup as the cross-entropy loss. Training is performed during 5 epochs with 100 training iterations per epoch. We evaluate the model on a test set of size 25% of the dataset. For each dataset, we run our experiments with 9 different seeds and display the average and the standard deviation of the test accuracy (both in %) over the 9 trials.



---
tags:
  - promptfoo
  - llamafiles
---

J'ai testé ce répo (hier) :
https://github.com/Mozilla-Ocho/llamafile

Possibilité création et execution de modèles pour l'inférence & le finetunning (je n'ai pas testé le finetunning) de manière efficace.

Possibilité de faire tourner sur CPU ou GPU (la rapidité d'inférence sur GPU est particulièrement notable)

Création d'un serveur d'API, reproduisant globalement l'API de OPENAI.

C'est là le premier "point bloquant" c'est que pour la vision ce n'est pas exactement l'API d'OPENAI donc pas utilisable avec promptfoo directement (il faudrait lancer un script intermédiaire et pas seulement promptfoo). Le problème a été remonté dans le github, normalement la version officiel de l'api image devrait etre implémenté mais il n'y a pas de date indiqué. L'avantage de la méthode utilisé c'est que les images sont envoyé encodé en base64 et pas juste l'url de l'image. Donc on peut utiliser le serveur d'API en circuit fermé sans internet.

On peut le faire tourner sur tous les modèles avec des fichiers GGUF (compatible llama.cpp).

Pour certains modèles, ces fichiers sont déjà créés, mais pour d'autres, il faut les créer, normalement, c'est possible avec tous les modèles, mais pas forcément évident pour tous (certaines solution testé en avril s'appuyait sur cette technologie mais elle avait été exclu car ce n'était pas évident de convertir les modèles). 

Correction des bugs dans le script de remplacement promptfoo

